{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"UPGXgFSll6Su"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def initialize_parameters(vocab_size, hidden_layer_size):\n","\n","    parameters = {}\n","    parameters[\"Whh\"] = np.random.randn(\n","        hidden_layer_size, hidden_layer_size) * 0.01\n","    parameters[\"Wxh\"] = np.random.randn(hidden_layer_size, vocab_size) * 0.01\n","    parameters[\"b\"] = np.zeros((hidden_layer_size, 1))\n","    parameters[\"Why\"] = np.random.randn(vocab_size, hidden_layer_size) * 0.01\n","    parameters[\"c\"] = np.zeros((vocab_size, 1))\n","\n","    return parameters\n","\n","\n","def initialize_adam(parameters):\n","\n","    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n","    v = {}\n","    s = {}\n","\n","    for param_name in parameters_names:\n","        v[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n","        s[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n","\n","    return v, s\n","\n","\n","def initialize_rmsprop(parameters):\n","\n","    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n","    s = {}\n","\n","    for param_name in parameters_names:\n","        s[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n","\n","    return s\n","\n","\n","def softmax(z):\n","\n","    e_z = np.exp(z)\n","    probs = e_z / np.sum(e_z)\n","\n","    return probs\n","\n","\n","def rnn_forward(x, y, h_prev, parameters):\n","\n","    # Retrieve parameters\n","    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n","    Why, c = parameters[\"Why\"], parameters[\"c\"]\n","\n","    # Initialize inputs, hidden state, output, and probabilities dictionaries\n","    xs, hs, os, probs = {}, {}, {}, {}\n","\n","    # Initialize x0 to zero vector\n","    xs[0] = np.zeros((vocab_size, 1))\n","\n","    # Initialize loss and assigns h_prev to last hidden state in hs\n","    loss = 0\n","    hs[-1] = np.copy(h_prev)\n","\n","    # Forward pass: loop over all characters of the name\n","    for t in range(len(x)):\n","        # Convert to one-hot vector\n","        if t > 0:\n","            xs[t] = np.zeros((vocab_size, 1))\n","            xs[t][x[t]] = 1\n","        # Hidden state\n","        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n","        # Logits\n","        os[t] = np.dot(Why, hs[t]) + c\n","        # Probs\n","        probs[t] = softmax(os[t])\n","        # Loss\n","        loss -= np.log(probs[t][y[t], 0])\n","\n","    cache = (xs, hs, probs)\n","\n","    return loss, cache\n","\n","\n","def smooth_loss(loss, current_loss):\n","\n","    return 0.999 * loss + 0.001 * current_loss\n","\n","\n","def clip_gradients(gradients, max_value):\n","\n","    for grad in gradients.keys():\n","        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n","\n","    return gradients\n","\n","\n","def rnn_backward(y, parameters, cache):\n","\n","    # Retrieve xs, hs, and probs\n","    xs, hs, probs = cache\n","\n","    # Initialize all gradients to zero\n","    dh_next = np.zeros_like(hs[0])\n","\n","    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n","    grads = {}\n","    for param_name in parameters_names:\n","        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n","\n","    # Iterate over all time steps in reverse order starting from Tx\n","    for t in reversed(range(len(xs))):\n","        dy = np.copy(probs[t])\n","        dy[y[t]] -= 1\n","        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n","        grads[\"dc\"] += dy\n","        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n","        dhraw = (1 - hs[t] ** 2) * dh\n","        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n","        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n","        grads[\"db\"] += dhraw\n","        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n","        # Clip the gradients using [-5, 5] as the interval\n","        grads = clip_gradients(grads, 5)\n","    # Get the last hidden state\n","    h_prev = hs[len(xs) - 1]\n","\n","    return grads, h_prev\n","\n","\n","def update_parameters_with_adam(\n","        parameters, grads, v, s, t, learning_rate, beta1=0.9, beta2=0.999,\n","        epsilon=1e-8):\n","\n","    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n","    v_corrected = {}\n","    s_corrected = {}\n","\n","    for param_name in parameters_names:\n","        # Update the moving average of first gradient and squared gradient\n","        v[\"d\" + param_name] = beta1 * v[\"d\" + param_name] +\\\n","            (1 - beta1) * grads[\"d\" + param_name]\n","        s[\"d\" + param_name] = beta2 * s[\"d\" + param_name] +\\\n","            (1 - beta2) * np.square(grads[\"d\" + param_name])\n","\n","        # Compute the corrected-bias estimate of the moving averages\n","        v_corrected[\"d\" + param_name] = v[\"d\" + param_name] / (1 - beta1**t)\n","        s_corrected[\"d\" + param_name] = s[\"d\" + param_name] / (1 - beta2**t)\n","\n","        # update parameters\n","        parameters[param_name] -= (learning_rate *\n","                                   v_corrected[\"d\" + param_name])\\\n","            / (np.sqrt(s_corrected[\"d\" + param_name] + epsilon))\n","\n","    return parameters, v, s\n","\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    for param in parameters.keys():\n","        parameters[param] -= learning_rate * grads[\"d\" + param]\n","\n","    return parameters\n","\n","\n","def update_parameters_with_rmsprop(\n","        parameters, grads, s, beta=0.9, learning_rate=0.001, epsilon=1e-8):\n","\n","    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n","\n","    for param_name in parameters_names:\n","        # Update exponential weighted average of squared gradients\n","        s[\"d\" + param_name] = beta * s[\"d\" + param_name] +\\\n","            (1 - beta) * np.square(grads[\"d\" + param_name])\n","\n","        # Update parameters\n","        parameters[param_name] -= (learning_rate * grads[\"d\" + param_name])\\\n","            / (np.sqrt(s[\"d\" + param_name] + epsilon))\n","\n","    return parameters, s\n","\n","\n","def sample(parameters, idx_to_chars, chars_to_idx, n):\n","\n","    # Retrienve parameters, shapes, and vocab size\n","    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n","    Why, c = parameters[\"Why\"], parameters[\"c\"]\n","    n_h, n_x = Wxh.shape\n","    vocab_size = c.shape[0]\n","\n","    # Initialize a0 and x1 to zero vectors\n","    h_prev = np.zeros((n_h, 1))\n","    x = np.zeros((n_x, 1))\n","\n","    # Initialize empty sequence\n","    indices = []\n","    idx = -1\n","    counter = 0\n","    while (counter <= n and idx != chars_to_idx[\"\\n\"]):\n","        # Fwd propagation\n","        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n","        o = np.dot(Why, h) + c\n","        probs = softmax(o)\n","\n","        # Sample the index of the character using generated probs distribution\n","        idx = np.random.choice(vocab_size, p=probs.ravel())\n","\n","        # Get the character of the sampled index\n","        char = idx_to_chars[idx]\n","\n","        # Add the char to the sequence\n","        indices.append(idx)\n","\n","        # Update a_prev and x\n","        h_prev = np.copy(h)\n","        x = np.zeros((n_x, 1))\n","        x[idx] = 1\n","\n","        counter += 1\n","    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n","\n","    return sequence\n","\n","\n","def model(\n","        file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size,\n","        num_epochs=10, learning_rate=0.01):\n","\n","    # Get the data\n","    with open(file_path) as f:\n","        data = f.readlines()\n","    examples = [x.lower().strip() for x in data]\n","\n","    # Initialize parameters\n","    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n","\n","    # Initialize Adam parameters\n","    s = initialize_rmsprop(parameters)\n","\n","    # Initialize loss\n","    smoothed_loss = -np.log(1 / vocab_size) * 7\n","\n","    # Initialize hidden state h0 and overall loss\n","    h_prev = np.zeros((hidden_layer_size, 1))\n","    overall_loss = []\n","\n","    # Iterate over number of epochs\n","    for epoch in range(num_epochs):\n","        print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n","        print(f\"\\033[1m\\033[92m=======\")\n","\n","        # Sample one name\n","        print(f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n","            10).capitalize()}\"\"\")\n","        print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n","\n","        # Shuffle examples\n","        np.random.shuffle(examples)\n","\n","        # Iterate over all examples (SGD)\n","        for example in examples:\n","            x = [None] + [chars_to_idx[char] for char in example]\n","            y = x[1:] + [chars_to_idx[\"\\n\"]]\n","            # Fwd pass\n","            loss, cache = rnn_forward(x, y, h_prev, parameters)\n","            # Compute smooth loss\n","            smoothed_loss = smooth_loss(smoothed_loss, loss)\n","            # Bwd pass\n","            grads, h_prev = rnn_backward(y, parameters, cache)\n","            # Update parameters\n","            parameters, s = update_parameters_with_rmsprop(\n","                parameters, grads, s)\n","\n","        overall_loss.append(smoothed_loss)\n","\n","    return parameters, overall_loss"],"metadata":{"id":"5KYHbQlv4Vd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLKRfg23NtBh"},"outputs":[],"source":["def rnn_forward(x, y, h_prev, parameters):\n","    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n","    Why, c = parameters[\"Why\"], parameters[\"c\"]\n","    xs, hs, os, probs = {}, {}, {}, {}\n","    xs[0] = np.zeros((vocab_size, 1))\n","    loss = 0\n","    hs[-1] = np.copy(h_prev)\n","    for t in range(len(x)):\n","        if t > 0:\n","            xs[t] = np.zeros((vocab_size, 1))\n","            xs[t][x[t]] = 1\n","        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n","        os[t] = np.dot(Why, hs[t]) + c\n","        probs[t] = softmax(os[t])\n","        loss -= np.log(probs[t][y[t], 0])\n","    cache = (xs, hs, probs)\n","    return loss, cache"]},{"cell_type":"code","source":["def clip_gradients(gradients, max_value):\n","    for grad in gradients.keys():\n","        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n","    return gradients\n","\n","def rnn_backward(y, parameters, cache):\n","    # Retrieve xs, hs, and probs\n","    xs, hs, probs = cache\n","    \n","    # Initialize all gradients to zero\n","    dh_next = np.zeros_like(hs[0])\n","    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n","    grads = {}\n","    for param_name in parameters_names:\n","        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n","    \n","    # Iterate over all time steps in reverse order starting from Tx\n","    for t in reversed(range(len(xs))):\n","        dy = np.copy(probs[t])\n","        dy[y[t]] -= 1\n","        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n","        grads[\"dc\"] += dy\n","        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n","        dhraw = (1 - hs[t] ** 2) * dh\n","        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n","        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n","        grads[\"db\"] += dhraw\n","        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n","        # Clip the gradients using [-5, 5] as the interval\n","        grads = clip_gradients(grads, 5)\n","    \n","    # Get the last hidden state\n","    h_prev = hs[len(xs) - 1]\n","    return grads, h_prev"],"metadata":{"id":"fkotAXNTUbtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample(parameters, idx_to_chars, chars_to_idx, n):\n","    # Retrienve parameters, shapes, and vocab size\n","    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n","    Why, c = parameters[\"Why\"], parameters[\"c\"]\n","    n_h, n_x = Wxh.shape\n","    vocab_size = c.shape[0]\n","    \n","    # Initialize a0 and x1 to zero vectors\n","    h_prev = np.zeros((n_h, 1))\n","    x = np.zeros((n_x, 1))\n","    # Initialize empty sequence\n","    indices = []\n","    idx = -1\n","    counter = 0\n","    while (counter <= n and idx != chars_to_idx[\"\\n\"]):\n","        # Fwd propagation\n","        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n","        o = np.dot(Why, h) + c\n","        probs = softmax(o)\n","        \n","        # Sample the index of the character using generated probs distribution\n","        idx = np.random.choice(vocab_size, p=probs.ravel())\n","        \n","        # Get the character of the sampled index\n","        char = idx_to_chars[idx]\n","    \n","        # Add the char to the sequence\n","        indices.append(idx)\n","        \n","        # Update a_prev and x\n","        h_prev = np.copy(h)\n","        x = np.zeros((n_x, 1))\n","        x[idx] = 1\n","        \n","        counter += 1\n","    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n","    return sequence"],"metadata":{"id":"pBTlkkRDUKzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model(\n","        file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size,\n","        num_epochs=10, learning_rate=0.01):\n","    # Get the data\n","    with open(file_path) as f:\n","        data = f.readlines()\n","    examples = [x.lower().strip() for x in data]\n","\n","    # Initialize parameters\n","    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n","\n","    # Initialize Adam parameters\n","    s = initialize_rmsprop(parameters)\n","\n","    # Initialize loss\n","    smoothed_loss = -np.log(1 / vocab_size) * 7\n","\n","    # Initialize hidden state h0 and overall loss\n","    h_prev = np.zeros((hidden_layer_size, 1))\n","    overall_loss = []\n","\n","    # Iterate over number of epochs\n","    for epoch in range(num_epochs):\n","        print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n","        print(f\"\\033[1m\\033[92m=======\")\n","\n","        # Sample one name\n","        print(f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n","            10).capitalize()}\"\"\")\n","        print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n","\n","        # Shuffle examples\n","        np.random.shuffle(examples)\n","\n","        # Iterate over all examples (SGD)\n","        for example in examples:\n","            x = [None] + [chars_to_idx[char] for char in example]\n","            y = x[1:] + [chars_to_idx[\"\\n\"]]\n","            # Fwd pass\n","            loss, cache = rnn_forward(x, y, h_prev, parameters)\n","            # Compute smooth loss\n","            smoothed_loss = smooth_loss(smoothed_loss, loss)\n","            # Bwd pass\n","            grads, h_prev = rnn_backward(y, parameters, cache)\n","            # Update parameters\n","            parameters, s = update_parameters_with_rmsprop(\n","                parameters, grads, s)\n","\n","        overall_loss.append(smoothed_loss)\n","    return parameters, overall_loss"],"metadata":{"id":"3x02sBVSUlc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load names\n","data = open(\"rnn.txt\", \"r\").read()\n","\n","# Convert characters to lower case\n","data = data.lower()\n","\n","# Construct vocabulary using unique characters, sort it in ascending order,\n","# then construct two dictionaries that maps character to index and index to\n","# characters.\n","chars = list(sorted(set(data)))\n","chars_to_idx = {ch:i for i, ch in enumerate(chars)}\n","idx_to_chars = {i:ch for ch, i in chars_to_idx.items()}\n","\n","# Get the size of the data and vocab size\n","data_size = len(data)\n","vocab_size = len(chars_to_idx)\n","print(f\"There are {data_size} characters and {vocab_size} unique characters.\")\n","\n","# Fitting the model\n","parameters, loss = model(\"rnn.txt\", chars_to_idx, idx_to_chars, 100, vocab_size, 10, 0.01)\n","\n","# Plotting the loss\n","plt.plot(range(len(loss)), loss)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Smoothed loss\");"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-M6gQEBmUqoa","outputId":"dd653a2b-e6c5-4c8e-aab0-5f683a3bad79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 412 characters and 29 unique characters.\n","\u001b[1m\u001b[94mEpoch 0\n","\u001b[1m\u001b[92m=======\n","Sampled name: Kdwoxe\n","Smoothed loss: 23.5711\n","\n","\u001b[1m\u001b[94mEpoch 1\n","\u001b[1m\u001b[92m=======\n","Sampled name: ,kxie,ash o\n","Smoothed loss: 24.4425\n","\n","\u001b[1m\u001b[94mEpoch 2\n","\u001b[1m\u001b[92m=======\n","Sampled name: R gemq,cesd\n","Smoothed loss: 25.2220\n","\n","\u001b[1m\u001b[94mEpoch 3\n","\u001b[1m\u001b[92m=======\n","Sampled name: Rhfq.u' i e\n","Smoothed loss: 25.9821\n","\n","\u001b[1m\u001b[94mEpoch 4\n","\u001b[1m\u001b[92m=======\n","Sampled name: Kup wpndphh\n","Smoothed loss: 26.7269\n","\n","\u001b[1m\u001b[94mEpoch 5\n","\u001b[1m\u001b[92m=======\n","Sampled name: R.f'abqgta\n","Smoothed loss: 27.4535\n","\n","\u001b[1m\u001b[94mEpoch 6\n","\u001b[1m\u001b[92m=======\n","Sampled name: Ux mgaw\n","Smoothed loss: 28.1656\n","\n","\u001b[1m\u001b[94mEpoch 7\n","\u001b[1m\u001b[92m=======\n","Sampled name: Sypphdbee l\n","Smoothed loss: 28.8558\n","\n","\u001b[1m\u001b[94mEpoch 8\n","\u001b[1m\u001b[92m=======\n","Sampled name: Km\n","Smoothed loss: 29.5269\n","\n","\u001b[1m\u001b[94mEpoch 9\n","\u001b[1m\u001b[92m=======\n","Sampled name: Och''a\n","Smoothed loss: 30.1733\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":[],"metadata":{"id":"KUy1yQCAUumF"},"execution_count":null,"outputs":[]}]}